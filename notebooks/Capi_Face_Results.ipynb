{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capi-Face-Results.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcaY5WJEM9RB"
      },
      "source": [
        "!pip install wandb -qqq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiSPn3AEM3S0"
      },
      "source": [
        "import wandb\n",
        "api = wandb.Api()\n",
        "\n",
        "# Change oreilly-class/cifar to <entity/project-name>\n",
        "runs = api.runs(\"capi-bgu/capi\")\n",
        "summary_list = [] \n",
        "config_list = [] \n",
        "name_list = [] \n",
        "for run in runs: \n",
        "    # run.summary are the output key/values like accuracy.  We call ._json_dict to omit large files \n",
        "    summary_list.append(run.summary._json_dict) \n",
        "\n",
        "    # run.config is the input metrics.  We remove special values that start with _.\n",
        "    config_list.append({k:v for k,v in run.config.items() if not k.startswith('_')}) \n",
        "\n",
        "    # run.name is the name of the run.\n",
        "    name_list.append(run.name)       \n",
        "\n",
        "import pandas as pd \n",
        "summary_df = pd.DataFrame.from_records(summary_list) \n",
        "config_df = pd.DataFrame.from_records(config_list) \n",
        "name_df = pd.DataFrame({'name': name_list}) \n",
        "all_df = pd.concat([name_df, config_df,summary_df], axis=1)\n",
        "\n",
        "all_df.to_csv(\"project.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkNVlReTNS9o"
      },
      "source": [
        "test_acc = pd.concat([summary_df[[\"test accuracy\", \"test balanced accuracy\"]], name_df], axis=1)\n",
        "test_acc = test_acc.dropna()\n",
        "test_acc[\"is_pos\"] = test_acc[\"name\"].str.contains(\".*pos.*\", regex=True)\n",
        "test_acc[\"label type\"] = test_acc[\"is_pos\"].map({\n",
        "    True: \"positive/negative\",\n",
        "    False: \"categorical\"\n",
        "})\n",
        "for i in range(10):\n",
        "  test_acc[\"label type\"][i] = \"general\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvyT73_XNcgo"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set(rc={\"figure.figsize\":(20, 10)}) #width=3, #height=4\n",
        "\n",
        "sns.scatterplot(data=test_acc, x=\"test balanced accuracy\", y=\"test accuracy\", hue=\"label type\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2OAFzuBUWBx"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "test_err = pd.concat([summary_df[[\"test mean absolute error\", \"test mean squared error\"]], name_df], axis=1)\n",
        "test_err = test_err.dropna()\n",
        "test_err_ensemble = test_err[test_err[\"name\"].str.contains(\".*ensemble.*\", regex=True)]\n",
        "\n",
        "sns.scatterplot(data=test_err, x=\"test mean squared error\", y=\"test mean absolute error\")\n",
        "sns.scatterplot(data=test_err_ensemble, x=\"test mean squared error\", y=\"test mean absolute error\")\n",
        "plt.legend([\"all models\", \"ensemble models\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYZa8OWrXWHx"
      },
      "source": [
        "import scipy.stats as sts\n",
        "\n",
        "test_mae = test_err[\"test mean absolute error\"].values\n",
        "test_mae_ensemble = test_err_ensemble[\"test mean absolute error\"].values\n",
        "print(sts.norm.interval(0.95, loc=test_mae.mean(), scale=test_mae.std()))\n",
        "print(sts.norm.interval(0.95, loc=test_mae_ensemble.mean(), scale=test_mae_ensemble.std()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L84C4Qtn6rCl"
      },
      "source": [
        "test_cm = pd.concat([summary_df[[\"ensemble confusion matrix_table\", \"testing confusion matrix_table\"]], name_df], axis=1)\n",
        "test_cm[\"testing confusion matrix_table\"][50]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}