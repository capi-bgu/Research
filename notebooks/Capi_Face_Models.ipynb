{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capi-Face-Models.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DWKoAN0cii9"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FQJuBnoch6u"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx0k9U2vhWRs"
      },
      "source": [
        "!pip install wandb -qqq\n",
        "!wandb login\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_ENTITY\"] = \"capi-bgu\"\n",
        "os.environ['WANDB_CONSOLE'] = \"off\"\n",
        "os.environ['WANDB_SILENT'] = \"true\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYWVGLjyfpdt"
      },
      "source": [
        "!pip install git+https://github.com/capi-bgu/oratio.git -qqq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvzjXzFKfP5d"
      },
      "source": [
        "!git clone https://github.com/capi-bgu/Research.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gDD_YBvcccx"
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import cv2\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from Research.util.data_loading import *\n",
        "from Research.util.evaluate import *\n",
        "from Research.models.emotionnet_nano import *\n",
        "from Research.models.frame_attention import *\n",
        "from oratio.Session import Session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxhnLffv4vNW"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itVbNfgCc7Kj"
      },
      "source": [
        "def create_dataset(dataframe, sequence_length=None, resize_to=None, labels=(\"categorical\"), test_split=0.1, val_split=None, random_state=None, category_encoder=None):\n",
        "  if category_encoder is None:\n",
        "    category_encoder = LabelEncoder()\n",
        "    category_encoder.fit(dataframe[\"categorical\"])\n",
        "  dataframe[\"categorical\"] = category_encoder.transform(dataframe[\"categorical\"])\n",
        "  raw_labels = dataframe[labels].values.astype(np.int)\n",
        "  raw_data = dataframe[\"images\"].values\n",
        "\n",
        "  for i, images in enumerate(raw_data):\n",
        "    if resize_to is not None:\n",
        "      images = np.array(images)\n",
        "      resized_images = np.zeros(shape=((len(images), ) + resize_to))\n",
        "      for j, image in enumerate(images):  \n",
        "        resized_images[j] = cv2.resize(image, resize_to)\n",
        "      raw_data[i] = resized_images\n",
        "    else:\n",
        "      raw_data[i] = np.array(images)\n",
        "  \n",
        "  train_data, test_data, train_labels, test_labels = train_test_split(raw_data,\n",
        "                                                                      raw_labels,\n",
        "                                                                      test_size=test_split,\n",
        "                                                                      random_state=random_state)\n",
        "  if val_split is not None:\n",
        "    train_data, val_data, train_labels, val_labels = train_test_split(train_data,\n",
        "                                                                      train_labels,\n",
        "                                                                      test_size=val_split,\n",
        "                                                                      random_state=random_state)\n",
        "\n",
        "  if sequence_length is not None:\n",
        "    unwraped_train_data = generate_sequences(train_data[0], sequence_length)\n",
        "    unwraped_test_data = generate_sequences(test_data[0], sequence_length)\n",
        "    if val_split is not None:\n",
        "      unwraped_val_data = generate_sequences(val_data[0], sequence_length)\n",
        "  else:\n",
        "    unwraped_train_data = train_data[0]\n",
        "    unwraped_test_data = test_data[0]\n",
        "    if val_split is not None:\n",
        "      unwraped_val_data = test_data[0]\n",
        "\n",
        "  unwraped_train_labels = np.repeat(np.expand_dims(train_labels[0], axis=0), len(unwraped_train_data), axis=0)\n",
        "  unwraped_test_labels = np.repeat(np.expand_dims(test_labels[0], axis=0), len(unwraped_test_data), axis=0)\n",
        "  if val_split is not None:\n",
        "    unwraped_val_labels = np.repeat(np.expand_dims(val_labels[0], axis=0), len(unwraped_val_data), axis=0)\n",
        "\n",
        "  \n",
        "  for i in range(1, len(train_data)):\n",
        "    if sequence_length is not None:\n",
        "      train_examples = generate_sequences(train_data[i], sequence_length)\n",
        "      if train_examples is None:\n",
        "        continue\n",
        "    else:\n",
        "      train_examples = train_data[i]\n",
        "    unwraped_train_data = np.concatenate((unwraped_train_data, train_examples))\n",
        "    unwraped_train_labels = np.concatenate(\n",
        "        (\n",
        "          unwraped_train_labels,\n",
        "          np.repeat(np.expand_dims(train_labels[i], axis=0), len(train_examples), axis=0)\n",
        "        )\n",
        "    )\n",
        "\n",
        "  for i in range(1, len(test_data)):\n",
        "    if sequence_length is not None:\n",
        "      test_examples = generate_sequences(test_data[i], sequence_length)\n",
        "      if test_examples is None:\n",
        "        continue\n",
        "    else:\n",
        "      test_examples = test_data[i]\n",
        "    unwraped_test_data = np.concatenate((unwraped_test_data, test_examples))\n",
        "    unwraped_test_labels = np.concatenate(\n",
        "      (\n",
        "        unwraped_test_labels,\n",
        "        np.repeat(np.expand_dims(test_labels[i], axis=0), len(test_examples), axis=0)\n",
        "      )\n",
        "    )\n",
        "  \n",
        "  if val_split is not None:\n",
        "    for i in range(1, len(val_data)):\n",
        "      if sequence_length is not None:\n",
        "        val_examples = generate_sequences(val_data[i], sequence_length)\n",
        "        if val_examples is None:\n",
        "          continue\n",
        "      else:\n",
        "        val_examples = val_data[i]\n",
        "      unwraped_val_data = np.concatenate((unwraped_val_data, val_examples))\n",
        "      unwraped_val_labels = np.concatenate(\n",
        "        (\n",
        "          unwraped_val_labels,\n",
        "          np.repeat(np.expand_dims(val_labels[i], axis=0), len(val_examples), axis=0)\n",
        "        )\n",
        "      )\n",
        "      \n",
        "  res = (unwraped_train_data / 255, unwraped_test_data / 255)\n",
        "  if val_split is not None:\n",
        "    res += (unwraped_val_data / 255, )\n",
        "  \n",
        "  res += (unwraped_train_labels, unwraped_test_labels)\n",
        "  if val_split is not None:\n",
        "    res += (unwraped_val_labels, )\n",
        "\n",
        "  if \"categorical\" in labels:\n",
        "    return res, category_encoder\n",
        "  \n",
        "  return res\n",
        "\n",
        "def generate_sequences(arr, sequence_length):\n",
        "  if len(arr) < sequence_length:\n",
        "    return None\n",
        "  sequences = np.zeros(shape=(len(arr) - (sequence_length - 1), sequence_length) + arr.shape[1:])\n",
        "  for i in range(len(arr) - (sequence_length - 1)):\n",
        "    sequences[i] = arr[i:i+sequence_length]\n",
        "  \n",
        "  return sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLO8oGun4ZiD"
      },
      "source": [
        "# EmotionNet Nano "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF30sJOlypBG"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLbv-lC6yzEh"
      },
      "source": [
        "import gc\n",
        "\n",
        "def use_bin(model):\n",
        "  last_layer = model.layers[-2].output\n",
        "  pred = keras.layers.Dense(2, activation=\"softmax\")(last_layer)\n",
        "  return keras.Model(inputs=model.inputs, outputs=pred)\n",
        "\n",
        "def use_reg(model):\n",
        "  last_layer = model.layers[-2].output\n",
        "  pred = keras.layers.Dense(3)(last_layer)\n",
        "  return keras.Model(inputs=model.inputs, outputs=pred)\n",
        "\n",
        "class ENNEvaluator(Evaluator):\n",
        "\n",
        "  def __init__(self, project, name, task=\"categorical\", use_logger=False,\n",
        "               use_ensemble=True, model_path: str = None, scorer: Scorer = None):\n",
        "    \n",
        "    self.loss = \"sparse_categorical_crossentropy\"\n",
        "    self.metrics = [\"accuracy\"]\n",
        "\n",
        "\n",
        "    if task == \"categorical\":\n",
        "      super(ENNEvaluator, self).__init__(project, name, use_logger=use_logger,\n",
        "                                         model_path=model_path, scorer=scorer)\n",
        "      self.model_pred = lambda x: x\n",
        "\n",
        "    elif task == \"positive\":\n",
        "      super(ENNEvaluator, self).__init__(project, name, use_logger=use_logger,\n",
        "                                         model_path=model_path, scorer=scorer)\n",
        "      self.model_pred = use_bin\n",
        "\n",
        "    elif task == \"regression\":\n",
        "      super(ENNEvaluator, self).__init__(project, name, use_logger=use_logger,\n",
        "                                         task=Task.REG, model_path=model_path, scorer=scorer)\n",
        "      self.model_pred = use_reg\n",
        "      self.loss = \"mse\"\n",
        "      self.metrics = [\"mae\"]\n",
        "\n",
        "  def model_config(self):\n",
        "    return {\n",
        "        \"optimizer\": 'adam',\n",
        "        \"loss\": self.loss,\n",
        "        \"metrics\": self.metrics,\n",
        "        \"batch_size\": 100,\n",
        "        \"epochs\": 100,\n",
        "    }\n",
        "\n",
        "  def train(self, train_data, validation_data, callbacks):\n",
        "    config = self.model_config()\n",
        "    model = emotion_nano_b()\n",
        "    model = self.model_pred(model)\n",
        "    model.compile(optimizer=config[\"optimizer\"], loss=config[\"loss\"], metrics=config[\"metrics\"])\n",
        "    model.fit(train_data[0], train_data[1], validation_data=validation_data,\n",
        "              batch_size=config[\"batch_size\"], epochs=config[\"epochs\"],\n",
        "              callbacks=callbacks)\n",
        "\n",
        "    gc.collect()\n",
        "    return model\n",
        "\n",
        "  def predict(self, model, data):\n",
        "    return model(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdjfdBvc-kDQ"
      },
      "source": [
        "import traceback\n",
        "import gc\n",
        "\n",
        "for name in (\"ron\", \"yuval\", \"tal\", \"liraz\", \"shiran\", \"tomer\", \"liraz\", \"niv\"):\n",
        "  for duration in (1, 5, 10):\n",
        "    features = load_features(f\"/content/drive/MyDrive/capi/data/public/{name}.db\", \"Camera\", duration)\n",
        "\n",
        "    try:\n",
        "      base_path = \"/content/drive/MyDrive/capi/models/frame_attention\"\n",
        "      \n",
        "      # categorical\n",
        "      gc.collect()\n",
        "      (train_data, test_data, train_labels, test_labels), category_encoder = create_dataset(features, resize_to=(48, 48), test_split=0.2, random_state=42)\n",
        "      gc.collect()\n",
        "      label_map = {k: v for k, v in zip(category_encoder.transform(category_encoder.classes_), category_encoder.classes_)}\n",
        "      cat_eval = ENNEvaluator(project=\"capi\", name=f\"emotionnet-nano_cat_{name}-{duration}d\",\n",
        "                              use_logger=True, model_path=f\"{base_path}/emotionnet-nano_cat-{name}-{duration}/\")\n",
        "      cat_eval.evaluate((train_data, train_labels), (test_data, test_labels), label_map, splits=10)\n",
        "\n",
        "      # positive\n",
        "      del train_data, test_data, train_labels, test_labels, cat_eval\n",
        "      gc.collect()\n",
        "      (train_data, test_data, train_labels, test_labels) = create_dataset(features, resize_to=(48, 48),\n",
        "                                                                          test_split=0.2, random_state=42,\n",
        "                                                                          labels=\"positive\")\n",
        "      gc.collect()\n",
        "      label_map = {0: \"negative\", 1: \"positive\"}\n",
        "      pos_eval = ENNEvaluator(project=\"capi\", name=f\"emotionnet-nano_pos_{name}-{duration}d\",\n",
        "                              use_logger=True, model_path=f\"{base_path}/emotionnet-nano_pos-{name}-{duration}/\",\n",
        "                              task=\"positive\")\n",
        "      pos_eval.evaluate((train_data, train_labels), (test_data, test_labels), label_map, splits=10)\n",
        "\n",
        "      # regression\n",
        "      del train_data, test_data, train_labels, test_labels, pos_eval\n",
        "      gc.collect()\n",
        "      (train_data, test_data, train_labels, test_labels) = create_dataset(features, resize_to=(48, 48),\n",
        "                                                                          test_split=0.2, random_state=42,\n",
        "                                                                          labels=[\"valance\", \"arousal\", \"dominance\"])\n",
        "      reg_eval = ENNEvaluator(project=\"capi\", name=f\"emotionnet-nano_reg_{name}-{duration}d\",\n",
        "                              use_logger=True, model_path=f\"{base_path}/emotionnet-nano_reg-{name}-{duration}/\",\n",
        "                              task=\"regression\")\n",
        "      reg_eval.evaluate((train_data, train_labels), (test_data, test_labels), splits=10)\n",
        "      del train_data, test_data, train_labels, test_labels, reg_eval\n",
        "      gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"-----------------ERROR-{name}-----------------\")\n",
        "      print(e)\n",
        "      print(traceback.print_exception(type(e), e, e.__traceback__))\n",
        "      print(f\"---------------END-ERROR-{name}---------------\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUIKspR6NCEJ"
      },
      "source": [
        "all = (\"ron\", \"yuval\", \"liraz\", \"shiran\", \"tomer\", \"niv\", \"tal\")\n",
        "base_path = \"/content/drive/MyDrive/capi/models/frame_attention\"\n",
        "duration = 10\n",
        "\n",
        "train_names, (test_name, ) = train_test_split(all, test_size=0.1, random_state=42)\n",
        "train_features = [load_features(f\"/content/drive/MyDrive/capi/data/public/{name}.db\", \"Camera\", duration) for name in train_names]\n",
        "test_features = load_features(f\"/content/drive/MyDrive/capi/data/public/{test_name}.db\", \"Camera\", duration)\n",
        "(_, test_data, _, test_labels), category_encoder = create_dataset(test_features, resize_to=(48, 48), test_split=0.99, random_state=42)\n",
        "\n",
        "(train_data, curr_test_data, train_labels, curr_test_labels), category_encoder = create_dataset(train_features[0], resize_to=(48, 48),\n",
        "                                                                                                test_split=0.05, random_state=42,\n",
        "                                                                                                category_encoder=category_encoder)\n",
        "\n",
        "test_data = np.concatenate((test_data, curr_test_data), axis=0)\n",
        "test_labels = np.concatenate((test_labels, curr_test_labels), axis=0)\n",
        "\n",
        "for i in range(1, len(train_features)):\n",
        "  testee_features = train_features[i]\n",
        "  (curr_train_data, curr_test_data, curr_train_labels, curr_test_labels), category_encoder = create_dataset(testee_features, resize_to=(48, 48),\n",
        "                                                                                          test_split=0.05, random_state=42,\n",
        "                                                                                          category_encoder=category_encoder)\n",
        "\n",
        "  train_data = np.concatenate((train_data, curr_train_data), axis=0)\n",
        "  train_labels = np.concatenate((train_labels, curr_train_labels), axis=0)\n",
        "  test_data = np.concatenate((test_data, curr_test_data), axis=0)\n",
        "  test_labels = np.concatenate((test_labels, curr_test_labels), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXaKEDkFwefx"
      },
      "source": [
        "cat_eval = ENNEvaluator(project=\"capi\", name=f\"emotionnet-nano_cat_general_{duration}d\",\n",
        "                              use_logger=True, model_path=f\"{base_path}/emotionnet-nano_cat-general-{duration}/\")\n",
        "label_map = {k: v for k, v in zip(category_encoder.transform(category_encoder.classes_), category_encoder.classes_)}\n",
        "cat_eval.evaluate((train_data, train_labels), (test_data, test_labels), label_map, splits=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zbwQQhWyn_b"
      },
      "source": [
        "# Frame Attention Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X19XqHGAWWm"
      },
      "source": [
        "camera_features = load_features(\"/content/drive/MyDrive/capi/data/public/ron.db\", \"Camera\", 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5I_0AaayvGy"
      },
      "source": [
        "(train_data, test_data, val_data, train_labels, test_labels, val_labels), category_encoder = create_dataset(camera_features, sequence_length=5, resize_to=(48, 48), test_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgD3cU8fyytq"
      },
      "source": [
        "frame_attention_model = fanet()\n",
        "frame_attention_model.compile(optimizer=keras.optimizers.Adam(0.1), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "def lr_schedular(epoch, lr):\n",
        "  if epoch == 10:\n",
        "    return lr * 1e-1\n",
        "  if epoch == 20:\n",
        "    return lr * 1e-2\n",
        "  if epoch == 30:\n",
        "    return lr * 1e-3\n",
        "\n",
        "  return lr\n",
        "\n",
        "callbacks = [\n",
        "  keras.callbacks.ModelCheckpoint(\"frame_attention/\", monitor='val_accuracy', save_best_only=True, save_weights_only=True),\n",
        "  keras.callbacks.LearningRateScheduler(lr_schedular)\n",
        "]\n",
        "frame_attention_model.fit(np.expand_dims(train_data, axis=4), train_labels, validation_data=(val_data, val_labels), batch_size=64, epochs=60, callbacks=callbacks)\n",
        "frame_attention_model.save_weights(\"frame_attention_last/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIgoshjW99Pm"
      },
      "source": [
        "frame_attention_model.load_weights(\"frame_attention_last/\")\n",
        "frame_attention_model.evaluate(np.expand_dims(test_data, axis=4), test_labels, batch_size=10)\n",
        "frame_attention_model.evaluate(np.expand_dims(val_data, axis=4), val_labels, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSVQaSzFO8pP"
      },
      "source": [
        "frame_attention_model.load_weights(\"frame_attention/\")\n",
        "frame_attention_model.evaluate(np.expand_dims(test_data, axis=4), test_labels, batch_size=10)\n",
        "frame_attention_model.evaluate(np.expand_dims(val_data, axis=4), val_labels, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}